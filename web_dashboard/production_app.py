"""Simplified production app for Azure deployment and local testing.This version removes Docker dependency for easier local testing."""import osimport jsonimport asyncioimport astimport sysimport tempfileimport subprocessfrom typing import Dict, List, Any, Optionalfrom pathlib import Pathimport logging# Web frameworkfrom fastapi import FastAPI, Request, Form, UploadFile, File, HTTPExceptionfrom fastapi.templating import Jinja2Templatesfrom fastapi.staticfiles import StaticFilesfrom fastapi.responses import HTMLResponse, JSONResponseimport uvicorn# Security analysistry:    import bandit    from bandit.core import config as bandit_config    from bandit.core import manager as bandit_manager    BANDIT_AVAILABLE = Trueexcept ImportError:    BANDIT_AVAILABLE = False# LLM clients  try:    import openai    OPENAI_AVAILABLE = Trueexcept ImportError:    OPENAI_AVAILABLE = Falsetry:    import anthropic    ANTHROPIC_AVAILABLE = Trueexcept ImportError:    ANTHROPIC_AVAILABLE = False# Setup logginglogging.basicConfig(level=logging.INFO)logger = logging.getLogger(__name__)class ProductionCodeEvaluator:    """Production-ready code evaluator with real analysis capabilities."""        def __init__(self):        self.llm_clients = self._setup_llm_clients()        logger.info(f"Initialized with {len(self.llm_clients)} LLM clients")            def _setup_llm_clients(self) -> Dict[str, Any]:        """Setup available LLM clients."""        clients = {}                # OpenAI client        if OPENAI_AVAILABLE and os.getenv("OPENAI_API_KEY"):            clients["openai"] = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))            logger.info("OpenAI client configured")                    # Azure OpenAI client        if OPENAI_AVAILABLE and os.getenv("AZURE_OPENAI_ENDPOINT"):            clients["azure"] = openai.AzureOpenAI(                azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),                api_key=os.getenv("AZURE_OPENAI_API_KEY"),                api_version="2024-02-01"            )            logger.info("Azure OpenAI client configured")                    # Anthropic client        if ANTHROPIC_AVAILABLE and os.getenv("ANTHROPIC_API_KEY"):            clients["anthropic"] = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))            logger.info("Anthropic client configured")                    return clients        def run_ast_analysis(self, code: str) -> Dict[str, Any]:        """Run AST-based static analysis."""        try:            tree = ast.parse(code)                        # Count various constructs            functions = len([n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)])            classes = len([n for n in ast.walk(tree) if isinstance(n, ast.ClassDef)])            imports = len([n for n in ast.walk(tree) if isinstance(n, (ast.Import, ast.ImportFrom))])            complexity = len([n for n in ast.walk(tree) if isinstance(n, (ast.If, ast.For, ast.While, ast.Try))])                        # Calculate basic metrics            lines = len(code.split('\n'))            non_empty_lines = len([l for l in code.split('\n') if l.strip()])                        # Score based on code structure            structure_score = min(1.0, (functions + classes) / max(1, lines / 10))            complexity_score = min(1.0, complexity / max(1, functions))            organization_score = min(1.0, imports / max(1, (functions + classes)))                        overall_score = (structure_score + complexity_score + organization_score) / 3                        return {                "score": overall_score,                "confidence": 0.8,                "metrics": {                    "functions": functions,                    "classes": classes,                    "imports": imports,                    "complexity": complexity,                    "lines": lines,                    "non_empty_lines": non_empty_lines                },                "feedback": f"Code structure analysis: {functions} functions, {classes} classes, complexity factor {complexity}"            }                    except SyntaxError as e:            return {                "score": 0.0,                "confidence": 1.0,                "error": f"Syntax error: {str(e)}",                "feedback": "Code contains syntax errors that prevent analysis"            }        except Exception as e:            return {                "score": 0.5,                "confidence": 0.3,                "error": f"Analysis error: {str(e)}",                "feedback": "Could not complete AST analysis due to unexpected error"            }        def run_bandit_analysis(self, code: str) -> Dict[str, Any]:        """Run Bandit security analysis."""        if not BANDIT_AVAILABLE:            return {                "score": 0.5,                "confidence": 0.0,                "feedback": "Bandit security scanner not available",                "issues": []            }                    try:            # Create temporary file for bandit            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:                f.write(code)                temp_file = f.name                        # Run bandit analysis            conf = bandit_config.BanditConfig()            b_mgr = bandit_manager.BanditManager(conf, 'file')            b_mgr.discover_files([temp_file])            b_mgr.run_tests()                        # Process results            issues = []            total_severity = 0            for result in b_mgr.get_issue_list():                severity_map = {"LOW": 1, "MEDIUM": 2, "HIGH": 3}                severity_score = severity_map.get(result.severity, 1)                total_severity += severity_score                                issues.append({                    "test_id": result.test_id,                    "severity": result.severity,                    "confidence": result.confidence,                    "text": result.text,                    "line_number": result.lineno                })                        # Calculate security score (higher is better)            if not issues:                score = 1.0            else:                # Penalize based on severity                max_possible_severity = len(issues) * 3  # Assuming all HIGH                score = max(0.0, 1.0 - (total_severity / max(max_possible_severity, 1)))                        # Cleanup            os.unlink(temp_file)                        return {                "score": score,                "confidence": 0.9,                "issues": issues,                "feedback": f"Security analysis found {len(issues)} potential issues"            }                    except Exception as e:            if 'temp_file' in locals():                try:                    os.unlink(temp_file)                except:                    pass            return {                "score": 0.5,                "confidence": 0.3,                "error": f"Security analysis error: {str(e)}",                "feedback": "Could not complete security analysis"            }        async def evaluate_communication(self, code: str, problem_description: str) -> Dict[str, Any]:        """Evaluate code communication using LLM."""        if not self.llm_clients:            return {                "score": 0.5,                "confidence": 0.0,                "feedback": "No LLM clients available for communication evaluation",                "mock": True            }                prompt = f"""        Evaluate the communication quality of this code solution:                Problem: {problem_description}                Code:        ```python        {code}        ```                Rate the code's communication on a scale of 0.0 to 1.0 based on:        1. Code readability and clarity        2. Appropriate variable and function names        3. Helpful comments and documentation        4. Clear structure and organization        5. How well it communicates the solution approach                Respond with JSON: {{"score": 0.0-1.0, "reasoning": "explanation"}}        """                try:            # Try OpenAI first            if "openai" in self.llm_clients:                response = await asyncio.to_thread(                    self.llm_clients["openai"].chat.completions.create,                    model="gpt-3.5-turbo",                    messages=[{"role": "user", "content": prompt}],                    max_tokens=500,                    temperature=0.3                )                result_text = response.choices[0].message.content                            elif "azure" in self.llm_clients:                response = await asyncio.to_thread(                    self.llm_clients["azure"].chat.completions.create,                    model=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-35-turbo"),                    messages=[{"role": "user", "content": prompt}],                    max_tokens=500,                    temperature=0.3                )                result_text = response.choices[0].message.content                            elif "anthropic" in self.llm_clients:                response = await asyncio.to_thread(                    self.llm_clients["anthropic"].messages.create,                    model="claude-3-haiku-20240307",                    max_tokens=500,                    messages=[{"role": "user", "content": prompt}]                )                result_text = response.content[0].text                            else:                raise Exception("No available LLM client")                        # Parse JSON response            try:                result = json.loads(result_text.strip())                return {                    "score": float(result.get("score", 0.5)),                    "confidence": 0.8,                    "feedback": result.get("reasoning", "LLM evaluation completed"),                    "llm_used": list(self.llm_clients.keys())[0]                }            except json.JSONDecodeError:                # Fallback parsing                score = 0.5                if "score" in result_text:                    import re                    score_match = re.search(r'score["\s:]*([0-9.]+)', result_text)                    if score_match:                        score = float(score_match.group(1))                                return {                    "score": score,                    "confidence": 0.6,                    "feedback": result_text[:200] + "..." if len(result_text) > 200 else result_text,                    "llm_used": list(self.llm_clients.keys())[0]                }                        except Exception as e:            logger.error(f"LLM evaluation error: {e}")            return {                "score": 0.5,                "confidence": 0.3,                "error": f"LLM evaluation failed: {str(e)}",                "feedback": "Could not complete LLM-based communication evaluation"            }        async def evaluate_code(self, code: str, problem_description: str) -> Dict[str, Any]:        """Run comprehensive code evaluation."""        logger.info("Starting comprehensive code evaluation")                results = {}                # Run AST analysis        results["ast_analysis"] = self.run_ast_analysis(code)                # Run security analysis        results["security_analysis"] = self.run_bandit_analysis(code)                # Run communication evaluation        results["communication_analysis"] = await self.evaluate_communication(code, problem_description)                # Calculate overall score        scores = []        weights = []                if results["ast_analysis"]["score"] is not None:            scores.append(results["ast_analysis"]["score"])            weights.append(0.4)                    if results["security_analysis"]["score"] is not None:            scores.append(results["security_analysis"]["score"])            weights.append(0.3)                    if results["communication_analysis"]["score"] is not None:            scores.append(results["communication_analysis"]["score"])            weights.append(0.3)                if scores:            overall_score = sum(s * w for s, w in zip(scores, weights)) / sum(weights)        else:            overall_score = 0.0                results["overall"] = {            "score": overall_score,            "confidence": sum(r.get("confidence", 0) for r in results.values()) / len(results),            "feedback": f"Comprehensive evaluation completed with overall score: {overall_score:.2f}"        }                logger.info(f"Evaluation completed. Overall score: {overall_score:.2f}")        return results# Initialize FastAPI appapp = FastAPI(title="TrustworthyCodeLLM Production", version="1.0.0")# Setup templatestemplates_dir = Path(__file__).parent / "templates"templates_dir.mkdir(exist_ok=True)templates = Jinja2Templates(directory=str(templates_dir))# Global evaluator instanceevaluator = ProductionCodeEvaluator()@app.get("/", response_class=HTMLResponse)async def home(request: Request):    """Home page with evaluation interface."""    return templates.TemplateResponse("index.html", {"request": request})@app.get("/health")async def health_check():    """Health check endpoint for Azure."""    return {        "status": "healthy",        "llm_clients": len(evaluator.llm_clients),        "bandit_available": BANDIT_AVAILABLE,        "openai_available": OPENAI_AVAILABLE,        "anthropic_available": ANTHROPIC_AVAILABLE    }@app.post("/evaluate")async def evaluate_code_endpoint(    code: str = Form(...),    problem_description: str = Form(...)):    """Evaluate code submission."""    try:        results = await evaluator.evaluate_code(code, problem_description)        return JSONResponse(content=results)    except Exception as e:        logger.error(f"Evaluation error: {e}")        raise HTTPException(status_code=500, detail=str(e))@app.post("/upload")async def upload_file(file: UploadFile = File(...)):    """Handle file upload for evaluation."""    try:        content = await file.read()        code = content.decode('utf-8')                # Use filename as problem description if available        problem_description = f"Evaluate uploaded file: {file.filename}"                results = await evaluator.evaluate_code(code, problem_description)        return JSONResponse(content=results)    except Exception as e:        logger.error(f"File upload error: {e}")        raise HTTPException(status_code=500, detail=str(e))# Create basic HTML template if it doesn't existdef create_template():    """Create basic HTML template for the interface."""    template_content = '''<!DOCTYPE html><html lang="en"><head>    <meta charset="UTF-8">    <meta name="viewport" content="width=device-width, initial-scale=1.0">    <title>TrustworthyCodeLLM - Production</title>    <style>        body { font-family: Arial, sans-serif; max-width: 1200px; margin: 0 auto; padding: 20px; }        .header { text-align: center; margin-bottom: 30px; }        .form-group { margin-bottom: 20px; }        label { display: block; margin-bottom: 5px; font-weight: bold; }        textarea, input { width: 100%; padding: 10px; border: 1px solid #ddd; border-radius: 4px; }        button { background: #007bff; color: white; padding: 10px 20px; border: none; border-radius: 4px; cursor: pointer; }        button:hover { background: #0056b3; }        .results { margin-top: 30px; padding: 20px; background: #f8f9fa; border-radius: 4px; }        .score { font-size: 1.2em; font-weight: bold; margin: 10px 0; }        .error { color: #dc3545; }        .success { color: #28a745; }    </style></head><body>    <div class="header">        <h1>🔒 TrustworthyCodeLLM Production</h1>        <p>Real-time code evaluation with security analysis and LLM assessment</p>    </div>        <form id="evaluationForm">        <div class="form-group">            <label for="problem">Problem Description:</label>            <textarea id="problem" name="problem_description" rows="3" placeholder="Describe the coding problem or task..."></textarea>        </div>                <div class="form-group">            <label for="code">Code to Evaluate:</label>            <textarea id="code" name="code" rows="15" placeholder="Enter your Python code here..."></textarea>        </div>                <button type="submit">🚀 Evaluate Code</button>    </form>        <div id="results" class="results" style="display: none;">        <h3>Evaluation Results</h3>        <div id="resultsContent"></div>    </div>        <script>        document.getElementById('evaluationForm').addEventListener('submit', async function(e) {            e.preventDefault();                        const formData = new FormData(this);            const button = this.querySelector('button');            const results = document.getElementById('results');            const content = document.getElementById('resultsContent');                        button.disabled = true;            button.textContent = '⏳ Evaluating...';            results.style.display = 'block';            content.innerHTML = '<p>Running comprehensive analysis...</p>';                        try {                const response = await fetch('/evaluate', {                    method: 'POST',                    body: formData                });                                const data = await response.json();                                if (response.ok) {                    let html = '';                                        if (data.overall) {                        html += `<div class="score ${data.overall.score > 0.7 ? 'success' : data.overall.score > 0.4 ? '' : 'error'}">                            Overall Score: ${(data.overall.score * 100).toFixed(1)}%                        </div>`;                    }                                        if (data.ast_analysis) {                        html += `<h4>📊 Code Structure Analysis</h4>`;                        html += `<p>Score: ${(data.ast_analysis.score * 100).toFixed(1)}%</p>`;                        html += `<p>${data.ast_analysis.feedback}</p>`;                        if (data.ast_analysis.metrics) {                            html += `<p><small>Functions: ${data.ast_analysis.metrics.functions}, Classes: ${data.ast_analysis.metrics.classes}, Complexity: ${data.ast_analysis.metrics.complexity}</small></p>`;                        }                    }                                        if (data.security_analysis) {                        html += `<h4>🔒 Security Analysis</h4>`;                        html += `<p>Score: ${(data.security_analysis.score * 100).toFixed(1)}%</p>`;                        html += `<p>${data.security_analysis.feedback}</p>`;                        if (data.security_analysis.issues && data.security_analysis.issues.length > 0) {                            html += `<p><strong>Security Issues:</strong></p><ul>`;                            data.security_analysis.issues.forEach(issue => {                                html += `<li>${issue.severity}: ${issue.text} (Line ${issue.line_number})</li>`;                            });                            html += `</ul>`;                        }                    }                                        if (data.communication_analysis) {                        html += `<h4>💬 Communication Quality</h4>`;                        html += `<p>Score: ${(data.communication_analysis.score * 100).toFixed(1)}%</p>`;                        html += `<p>${data.communication_analysis.feedback}</p>`;                        if (data.communication_analysis.llm_used) {                            html += `<p><small>LLM Used: ${data.communication_analysis.llm_used}</small></p>`;                        }                    }                                        content.innerHTML = html;                } else {                    content.innerHTML = `<div class="error">Error: ${data.detail}</div>`;                }            } catch (error) {                content.innerHTML = `<div class="error">Network error: ${error.message}</div>`;            } finally {                button.disabled = false;                button.textContent = '🚀 Evaluate Code';            }        });    </script></body></html>    '''        templates_dir = Path(__file__).parent / "templates"    templates_dir.mkdir(exist_ok=True)        with open(templates_dir / "index.html", "w") as f:        f.write(template_content)# Create template on startupcreate_template()if __name__ == "__main__":    port = int(os.getenv("PORT", 8000))    uvicorn.run(        "production_app:app",        host="0.0.0.0",        port=port,        reload=False,        log_level="info"    )